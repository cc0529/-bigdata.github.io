<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Big data introduction</title>
    <link rel="stylesheet" href="stylesheet.css"
    <style></style>
</head>
<style>
body{
    width:800px;
    margin: 0 auto;
}
img{
    width:100%;
}
p,a {
    font-size: 15px;
    font-family: Arial, Helvetica, sans-serif
}
/* select certain id */
.fivev {
    font-size: 18px;
    font-family: Arial, Helvetica, sans-serif;
    margin: 0px ;
}
   </style>
</head>
<body>
<div>
    <h1 id="sec1" style="color:rgb(red, green, blue)">Big data introduction</h1>
    <hr./>
    <h2>Table of content</h2>
    <ul>
        <li><a href="#sec1">Introduction</a></li>
        <li><a href="#sec2">Description</a></li>
        <li><a href="#sec3">Characteristics</a></li>
    </ul>
    <img src="/big data.jpg"alt="intro to big data">
    <h2 id="sec1">Introduction</h2>
    <p><a href="https://en.wikipedia.org/wiki/Big_data">Big data</a>refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many fields (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.</p>
    <h2 id="#sec1"></h2>Introduction</h2>
    <hr/>
    <p>The size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks.The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.5×260 bytes) of data are generated.Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data.According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021.While Statista report, the global big data market is forecasted to grow to $103 billion by 2027.In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than €100 billion ($149 billion) in operational efficiency improvements alone by using big data.And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.
      Relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require "massively parallel software running on tens, hundreds, or even thousands of servers". What qualifies as "big data" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. "For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration."</p>

    <h2 id="sec2">Description</h2>
    <hr/>
    <p>The term big data has been in use since the 1990s, with some giving credit to John Mashey for popularizing the term. Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time.Big data philosophy encompasses unstructured, semi-structured and structured data; however, the main focus is on unstructured data.Big data "size" is a constantly moving target; as of 2012 ranging from a few dozen terabytes to many zettabytes of data.Big data requires a set of techniques and technologies with new forms of integration to reveal insights from data-sets that are diverse, complex, and of a massive scale.
        "Variety", "veracity", and various other "Vs" are added by some organizations to describe it, a revision challenged by some industry authorities.The Vs of big data were often referred to as the "three Vs", "four Vs", and "five Vs". They represented the qualities of big data in volume, variety, velocity, veracity, and value. Variability is often included as an additional quality of big data.
        A 2018 definition states "Big data is where parallel computing tools are needed to handle data", and notes, "This represents a distinct and clearly defined change in the computer science used, via parallel programming theories, and losses of some of the guarantees and capabilities made by Codd's relational model."</p>
</div>
<div></div>
    <h2 id="sec3">Characteristics</h2>
    <hr/>
    <p>Big data can be described by the following characteristics:</p>
</body>
</div>
</html>